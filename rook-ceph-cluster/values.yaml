rook-ceph-cluster:
  operatorNamespace: infra-storage
  clusterName: infra-ceph
  cephClusterSpec:
    dataDirHostPath: /var/lib/rook
    mon:
      # Set the number of mons to be started. Generally recommended to be 3.
      # For highest availability, an odd number of mons should be specified.
      count: 3
      # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
      # Mons should only be allowed on the same node for test environments where data loss is acceptable.
      allowMultiplePerNode: false
      # A volume claim template can be specified in which case new monitors (and
      # monitors created during fail over) will construct a PVC based on the
      # template for the monitor's primary storage. Changes to the template do not
      # affect existing monitors. Log data is stored on the HostPath under
      # dataDirHostPath. If no storage requirement is specified, a default storage
      # size appropriate for monitor data will be used.
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          resources:
            requests:
              storage: 10Gi
    cephVersion:
      image: quay.io/ceph/ceph:v19.2.3
      allowUnsupported: false
    skipUpgradeChecks: false
    continueUpgradeAfterChecksEvenIfNotHealthy: false
    mgr:
      count: 2
      allowMultiplePerNode: false
      modules:
        - name: rook
          enabled: true
    dashboard:
      enabled: true
      ssl: true
    crashCollector:
      disable: false
    logCollector:
      enabled: true
      periodicity: daily # one of: hourly, daily, weekly, monthly
      maxLogSize: 500M # SUFFIX may be 'M' or 'G'. Must be at least 1M.
    storage:
      allowDeviceClassUpdate: false # whether to allow changing the device class of an OSD after it is created
      allowOsdCrushWeightUpdate: true # whether to allow resizing the OSD crush weight after osd pvc is increased
      storageClassDeviceSets:
        - name: set1
          # The number of OSDs to create from this device set
          count: 3
          # IMPORTANT: If volumes specified by the storageClassName are not portable across nodes
          # this needs to be set to false. For example, if using the local storage provisioner
          # this should be false.
          portable: false
          # Certain storage class in the Cloud are slow
          # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
          tuneDeviceClass: true
          # Certain storage class in the Cloud are fast
          # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
          # Currently, "managed-premium" has been identified as such
          tuneFastDeviceClass: false
          # whether to encrypt the deviceSet or not
          encrypted: false
          # Since the OSDs could end up on any node, an effort needs to be made to spread the OSDs
          # across nodes as much as possible. Unfortunately the pod anti-affinity breaks down
          # as soon as you have more than one OSD per node. The topology spread constraints will
          # give us an even spread on K8s 1.18 or newer.
          placement:
            topologySpreadConstraints: []
          preparePlacement:
            podAntiAffinity: {}
            topologySpreadConstraints:
              - maxSkew: 1
                # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
                topologyKey: topology.kubernetes.io/zone
                whenUnsatisfiable: DoNotSchedule
          resources:
          # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources
          #   limits:
          #     memory: "4Gi"
          #   requests:
          #     cpu: "500m"
          #     memory: "4Gi"
          volumeClaimTemplates:
            - metadata:
                name: data
                labels:
                  app: rook-ceph-osd
              spec:
                resources:
                  requests:
                    storage: 10Gi
                storageClassName: longhorn
                volumeMode: Block
                accessModes:
                  - ReadWriteOnce
      onlyApplyOSDPlacement: false
    resources:
      prepareosd:
        requests:
          cpu: "200m"
          memory: "200Mi"
    priorityClassNames:
      mon: system-node-critical
      osd: system-node-critical
      mgr: system-cluster-critical
    disruptionManagement:
      managePodBudgets: true
      osdMaintenanceTimeout: 30
    security:
      keyRotation:
        schedule: "@monthly"
  cephObjectStores:
    - name: ceph-objectstore
      spec:
        metadataPool:
          failureDomain: host
          replicated:
            size: 3
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          parameters:
            bulk: "true"
        preservePoolsOnDelete: true
        gateway:
          port: 80
          resources:
            limits:
              memory: "2Gi"
            requests:
              cpu: "1000m"
              memory: "1Gi"
          securePort: 443
          sslCertificateRef: objectstore-tls
          instances: 1
          priorityClassName: system-cluster-critical
      storageClass:
        enabled: true
        name: ceph-bucket
        reclaimPolicy: Delete
        volumeBindingMode: "Immediate"
        annotations: { }
        labels: { }
        parameters:
          region: eu-east-1
      ingress:
        enabled: true
        ingressClassName: traefik
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt
        host:
          name: objectstore.karafra.net
          path: /
          pathType: Prefix
        tls:
          - hosts:
              - objectstore.karafra.net
            secretName: objectstore-tls
      route:
        enabled: false

cephObjectStoreUsers:
  - displayName: tempo-admin
    objectStoreName: ceph-objectstore
